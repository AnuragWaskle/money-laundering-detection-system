{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb2f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "PyTorch version: 2.5.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f89f4f",
   "metadata": {},
   "source": [
    " Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef2ff3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Using a sample of 49278 transactions for GNN training.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    df = pd.read_csv('../data/raw/PS_20174392719_1491204439457_log.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found.\")\n",
    "    df = None\n",
    "\n",
    "if df is not None:\n",
    "    # Use a smaller, balanced sample for faster training in this example\n",
    "    df_fraud = df[df['isFraud'] == 1]\n",
    "    df_normal = df[df['isFraud'] == 0].sample(n=len(df_fraud) * 5, random_state=42)\n",
    "    df_sample = pd.concat([df_fraud, df_normal]).reset_index(drop=True)\n",
    "    print(f\"Using a sample of {len(df_sample)} transactions for GNN training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a860500",
   "metadata": {},
   "source": [
    "Construct the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714d3311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing graph data structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hk908\\AppData\\Local\\Temp\\ipykernel_8592\\3590852168.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  edge_index = torch.tensor([source_nodes, dest_nodes], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data object created:\n",
      "Data(x=[96883, 1], edge_index=[2, 49278], edge_attr=[49278, 5], y=[49278])\n"
     ]
    }
   ],
   "source": [
    "if df is not None:\n",
    "    print(\"Constructing graph data structure...\")\n",
    "    \n",
    "    # Create a mapping from account names to integer indices\n",
    "    all_accounts = pd.concat([df_sample['nameOrig'], df_sample['nameDest']]).unique()\n",
    "    account_map = {name: i for i, name in enumerate(all_accounts)}\n",
    "    num_nodes = len(all_accounts)\n",
    "\n",
    "    # Create edge index\n",
    "    source_nodes = df_sample['nameOrig'].map(account_map).values\n",
    "    dest_nodes = df_sample['nameDest'].map(account_map).values\n",
    "    edge_index = torch.tensor([source_nodes, dest_nodes], dtype=torch.long)\n",
    "\n",
    "    # Create edge features (attributes)\n",
    "    edge_features = df_sample[['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']].values\n",
    "    scaler = StandardScaler()\n",
    "    edge_features = scaler.fit_transform(edge_features)\n",
    "    edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "    \n",
    "    # Create node features (using transaction amounts for simplicity)\n",
    "    # A more advanced approach would aggregate features per node\n",
    "    node_features = np.zeros((num_nodes, 1))\n",
    "    df_sample['amount_log'] = np.log1p(df_sample['amount'])\n",
    "    \n",
    "    # Aggregate features for each node (e.g., total amount sent/received)\n",
    "    sender_amounts = df_sample.groupby('nameOrig')['amount_log'].sum()\n",
    "    receiver_amounts = df_sample.groupby('nameDest')['amount_log'].sum()\n",
    "    \n",
    "    for name, idx in account_map.items():\n",
    "        node_features[idx, 0] = sender_amounts.get(name, 0) + receiver_amounts.get(name, 0)\n",
    "\n",
    "    node_features = scaler.fit_transform(node_features)\n",
    "    x = torch.tensor(node_features, dtype=torch.float)\n",
    "\n",
    "    # Create labels (for transactions/edges)\n",
    "    y = torch.tensor(df_sample['isFraud'].values, dtype=torch.float)\n",
    "\n",
    "    # Create the PyG Data object\n",
    "    graph_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)\n",
    "    \n",
    "    print(\"Graph data object created:\")\n",
    "    print(graph_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01484824",
   "metadata": {},
   "source": [
    "Define the GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2029d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_edge_features, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = SAGEConv(num_node_features, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        # Final layer predicts a single value per edge (transaction)\n",
    "        self.output_layer = torch.nn.Linear(2 * hidden_channels + num_edge_features, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # Get node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        # For each edge, concatenate the source node, dest node, and edge features\n",
    "        source_node_embed = x[edge_index[0]]\n",
    "        dest_node_embed = x[edge_index[1]]\n",
    "        \n",
    "        combined_features = torch.cat([source_node_embed, dest_node_embed, edge_attr], dim=1)\n",
    "        \n",
    "        # Predict\n",
    "        out = self.output_layer(combined_features)\n",
    "        return torch.sigmoid(out).squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb2a5f",
   "metadata": {},
   "source": [
    "Training the GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98cffe4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting GNN training...\n",
      "Epoch: 010, Loss: 0.3620\n",
      "Epoch: 020, Loss: 0.3350\n",
      "Epoch: 030, Loss: 0.3295\n",
      "Epoch: 040, Loss: 0.3247\n",
      "Epoch: 050, Loss: 0.3214\n",
      "Epoch: 060, Loss: 0.3189\n",
      "Epoch: 070, Loss: 0.3169\n",
      "Epoch: 080, Loss: 0.3153\n",
      "Epoch: 090, Loss: 0.3138\n",
      "Epoch: 100, Loss: 0.3124\n",
      "GNN training completed.\n"
     ]
    }
   ],
   "source": [
    "if 'graph_data' in locals():\n",
    "    model = GNN(\n",
    "        num_node_features=graph_data.num_node_features,\n",
    "        num_edge_features=graph_data.num_edge_features,\n",
    "        hidden_channels=64\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.BCELoss() # Binary Cross-Entropy Loss\n",
    "\n",
    "    # Create a mask for training (we're training on edges)\n",
    "    train_mask = torch.ones(graph_data.num_edges, dtype=torch.bool)\n",
    "\n",
    "    print(\"\\nStarting GNN training...\")\n",
    "    for epoch in range(100):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph_data.x, graph_data.edge_index, graph_data.edge_attr)\n",
    "        loss = criterion(out[train_mask], graph_data.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch: {epoch+1:03d}, Loss: {loss:.4f}')\n",
    "    print(\"GNN training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3852d4",
   "metadata": {},
   "source": [
    "Evaluate the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1859031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GNN Model Accuracy: 0.8867\n"
     ]
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(graph_data.x, graph_data.edge_index, graph_data.edge_attr)\n",
    "        preds = (out > 0.5).float()\n",
    "        correct = (preds == graph_data.y).sum()\n",
    "        accuracy = int(correct) / len(graph_data.y)\n",
    "        print(f'\\nGNN Model Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c0ef4",
   "metadata": {},
   "source": [
    "Save the GNN Model and Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb4c599d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GNN model saved to ../models/gnn_model.pt\n",
      "Account map saved to ../models/gnn_account_map.pkl\n"
     ]
    }
   ],
   "source": [
    "if 'model' in locals():\n",
    "    # Save the trained GNN model\n",
    "    model_path = '../models/gnn_model.pt'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"\\nGNN model saved to {model_path}\")\n",
    "\n",
    "    # Save the account map needed for inference\n",
    "    map_path = '../models/gnn_account_map.pkl'\n",
    "    joblib.dump(account_map, map_path)\n",
    "    print(f\"Account map saved to {map_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
